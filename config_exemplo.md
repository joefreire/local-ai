# üìã Configura√ß√µes do Sistema - Guia Completo

## üîß Arquivo .env

### Configura√ß√µes Obrigat√≥rias

```bash
# === MONGODB ===
# URL de conex√£o (Atlas ou local)
MONGODB_URL=mongodb+srv://usuario:senha@cluster.mongodb.net/
MONGODB_DATABASE=dashboard_whatsapp

# === WHISPER ===
# Modelo Whisper (recomendado: large-v3)
WHISPER_MODEL=large-v3
WHISPER_LANGUAGE=pt
```

### Configura√ß√µes de GPU

```bash
# === GPU SETTINGS ===
# Para RTX 4070 (8.6GB VRAM)
GPU_BATCH_SIZE=4
GPU_MEMORY_FRACTION=0.8

# Para RTX 3060 (6GB VRAM)
# GPU_BATCH_SIZE=2
# GPU_MEMORY_FRACTION=0.7

# Para RTX 4090 (24GB VRAM)
# GPU_BATCH_SIZE=8
# GPU_MEMORY_FRACTION=0.9
```

### Configura√ß√µes de Processamento

```bash
# === PROCESSAMENTO ===
# Workers paralelos (recomendado: 8)
MAX_CONCURRENT_JOBS=8

# Timeout para download (segundos)
AUDIO_DOWNLOAD_TIMEOUT=60

# Intervalo de processamento autom√°tico (segundos)
PROCESSING_INTERVAL=30
```

### Configura√ß√µes de An√°lise (Opcional)

```bash
# === OLLAMA ===
# URL do servidor Ollama
OLLAMA_BASE_URL=http://localhost:11434

# Modelo LLM
OLLAMA_MODEL=llama3.1:8b

# Para an√°lise mais r√°pida
# OLLAMA_MODEL=llama3.1:3b

# Para an√°lise mais precisa
# OLLAMA_MODEL=llama3.1:70b
```

## üéØ Configura√ß√µes por Cen√°rio

### Cen√°rio 1: Desenvolvimento/Teste

```bash
# Configura√ß√£o leve para desenvolvimento
WHISPER_MODEL=base
GPU_BATCH_SIZE=1
GPU_MEMORY_FRACTION=0.5
MAX_CONCURRENT_JOBS=2
LOG_LEVEL=DEBUG
```

### Cen√°rio 2: Produ√ß√£o (RTX 4070)

```bash
# Configura√ß√£o otimizada para RTX 4070
WHISPER_MODEL=large-v3
GPU_BATCH_SIZE=4
GPU_MEMORY_FRACTION=0.8
MAX_CONCURRENT_JOBS=8
LOG_LEVEL=INFO
```

### Cen√°rio 3: Alta Performance (RTX 4090)

```bash
# Configura√ß√£o para m√°xima performance
WHISPER_MODEL=large-v3
GPU_BATCH_SIZE=8
GPU_MEMORY_FRACTION=0.9
MAX_CONCURRENT_JOBS=16
LOG_LEVEL=INFO
```

### Cen√°rio 4: CPU Only

```bash
# Configura√ß√£o sem GPU
WHISPER_MODEL=base
GPU_BATCH_SIZE=1
GPU_MEMORY_FRACTION=0.0
MAX_CONCURRENT_JOBS=4
LOG_LEVEL=INFO
```

## üìä Modelos Whisper

| Modelo | Tamanho | VRAM | Velocidade | Qualidade |
|--------|---------|------|------------|-----------|
| `tiny` | 39 MB | 1 GB | Muito r√°pido | Baixa |
| `base` | 74 MB | 1 GB | R√°pido | Boa |
| `small` | 244 MB | 2 GB | M√©dio | Boa |
| `medium` | 769 MB | 5 GB | Lento | Muito boa |
| `large` | 1550 MB | 10 GB | Muito lento | Excelente |
| `large-v3` | 1550 MB | 10 GB | Muito lento | **Excelente** |

**Recomenda√ß√£o**: `large-v3` para RTX 4070

## üß† Modelos Ollama

| Modelo | Tamanho | RAM | Velocidade | Qualidade |
|--------|---------|-----|------------|-----------|
| `llama3.1:3b` | 2 GB | 4 GB | R√°pido | Boa |
| `llama3.1:8b` | 4.7 GB | 8 GB | M√©dio | **Muito boa** |
| `llama3.1:70b` | 40 GB | 80 GB | Lento | Excelente |

**Recomenda√ß√£o**: `llama3.1:8b` para an√°lise equilibrada

## üîç Verifica√ß√£o de Configura√ß√£o

### Teste de Configura√ß√£o

```powershell
# Testar configura√ß√£o atual
python cli_refactored.py test-all

# Verificar GPU especificamente
python cli_refactored.py test-gpu

# Verificar MongoDB
python cli_refactored.py test-mongodb
```

### Comandos de Verifica√ß√£o

```powershell
# Verificar CUDA
python -c "import torch; print(f'CUDA: {torch.cuda.is_available()}, Device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"N/A\"}')"

# Verificar Whisper
python -c "import whisper; print('Whisper OK')"

# Verificar Ollama
curl http://localhost:11434/api/tags
```

## ‚ö†Ô∏è Troubleshooting de Configura√ß√£o

### Problema: GPU n√£o detectada

**Solu√ß√£o:**
```bash
# Verificar CUDA
nvidia-smi

# Reinstalar PyTorch com CUDA
pip uninstall torch torchaudio
pip install torch torchaudio --index-url https://download.pytorch.org/whl/cu121
```

### Problema: Mem√≥ria GPU insuficiente

**Solu√ß√£o:**
```bash
# Reduzir batch size
GPU_BATCH_SIZE=2

# Reduzir uso de mem√≥ria
GPU_MEMORY_FRACTION=0.6

# Usar modelo menor
WHISPER_MODEL=base
```

### Problema: MongoDB n√£o conecta

**Solu√ß√£o:**
```bash
# Verificar URL
echo $MONGODB_URL

# Testar conex√£o
python -c "import pymongo; client = pymongo.MongoClient('$MONGODB_URL'); print('OK' if client.admin.command('ping') else 'ERRO')"
```

### Problema: Ollama n√£o responde

**Solu√ß√£o:**
```bash
# Verificar se est√° rodando
curl http://localhost:11434/api/tags

# Iniciar Ollama
ollama serve

# Baixar modelo
ollama pull llama3.1:8b
```

## üìà Otimiza√ß√µes de Performance

### Para RTX 4070

```bash
# Configura√ß√£o otimizada
WHISPER_MODEL=large-v3
GPU_BATCH_SIZE=4
GPU_MEMORY_FRACTION=0.8
MAX_CONCURRENT_JOBS=8
```

**Resultado esperado:**
- 50-100 √°udios/minuto
- 3000-6000 √°udios/hora
- <2min por √°udio

### Para RTX 3060

```bash
# Configura√ß√£o conservadora
WHISPER_MODEL=medium
GPU_BATCH_SIZE=2
GPU_MEMORY_FRACTION=0.7
MAX_CONCURRENT_JOBS=6
```

**Resultado esperado:**
- 30-60 √°udios/minuto
- 1800-3600 √°udios/hora
- <3min por √°udio

### Para CPU Only

```bash
# Configura√ß√£o CPU
WHISPER_MODEL=base
GPU_BATCH_SIZE=1
GPU_MEMORY_FRACTION=0.0
MAX_CONCURRENT_JOBS=4
```

**Resultado esperado:**
- 5-15 √°udios/minuto
- 300-900 √°udios/hora
- 5-10min por √°udio

## üîÑ Configura√ß√µes Din√¢micas

### Ajustar durante execu√ß√£o

```python
# No c√≥digo Python
from src.config import Config

# Ajustar batch size dinamicamente
Config.GPU_BATCH_SIZE = 2

# Ajustar workers
Config.MAX_CONCURRENT_JOBS = 4
```

### Monitoramento de recursos

```powershell
# Monitorar GPU
nvidia-smi -l 1

# Monitorar CPU/RAM
htop

# Monitorar logs
tail -f logs/processing.log
```

## üìù Logs e Debugging

### N√≠veis de Log

```bash
# Debug completo
LOG_LEVEL=DEBUG

# Informa√ß√µes normais
LOG_LEVEL=INFO

# Apenas erros
LOG_LEVEL=ERROR
```

### Localiza√ß√£o dos Logs

```
logs/
‚îú‚îÄ‚îÄ processing.log      # Logs de processamento
‚îú‚îÄ‚îÄ audio.log          # Logs de transcri√ß√£o
‚îú‚îÄ‚îÄ analysis.log       # Logs de an√°lise
‚îî‚îÄ‚îÄ system.log         # Logs do sistema
```

### Comandos de Debug

```powershell
# Ver logs em tempo real
tail -f logs/processing.log

# Filtrar erros
grep "ERROR" logs/*.log

# Ver √∫ltimas 100 linhas
tail -100 logs/processing.log
```
