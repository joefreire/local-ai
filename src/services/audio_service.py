"""
Service para processamento de √°udio com Whisper
"""
import torch
import whisper
import tempfile
import shutil
import json
from pathlib import Path
from typing import List, Dict, Optional, Tuple, Any
import time
from datetime import datetime

from .base_service import BaseService
from ..config import Config

class AudioService(BaseService):
    """Service para processamento de √°udio"""
    
    def _initialize(self):
        """Inicializar modelo Whisper"""
        self.device = self._setup_gpu()
        self.model = None
        self._load_whisper_model()
    
    def _setup_gpu(self) -> str:
        """Configurar GPU para processamento com fallback autom√°tico"""
        try:
            if torch.cuda.is_available():
                device = f"cuda:{torch.cuda.current_device()}"
                
                # Configurar mem√≥ria apenas se dispon√≠vel
                try:
                    torch.cuda.set_per_process_memory_fraction(Config.GPU_MEMORY_FRACTION)
                except Exception as e:
                    self.logger.warning(f"‚ö†Ô∏è N√£o foi poss√≠vel configurar mem√≥ria GPU: {e}")
                
                gpu_name = torch.cuda.get_device_name()
                total_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)
                
                self.logger.info(f"üöÄ GPU detectada: {gpu_name} ({total_memory:.1f}GB)")
                self.logger.info(f"üîß Mem√≥ria configurada: {Config.GPU_MEMORY_FRACTION*100}%")
                
                return device
            else:
                self.logger.info("üíª GPU n√£o dispon√≠vel - usando CPU (compat√≠vel com todos os sistemas)")
                return "cpu"
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è Erro ao detectar GPU: {e} - usando CPU")
            return "cpu"
    
    def _load_whisper_model(self):
        """Carregar modelo Whisper"""
        try:
            self.logger.info(f"üì• Carregando modelo Whisper: {Config.WHISPER_MODEL}")
            self.model = whisper.load_model(
                Config.WHISPER_MODEL,
                device=self.device,
                download_root=str(Config.MODELS_DIR)
            )
            self.logger.info("‚úÖ Modelo Whisper carregado")
        except Exception as e:
            self.logger.error(f"‚ùå Erro ao carregar modelo: {e}")
            raise
    
    def transcribe_file(self, file_path: str) -> Optional[Dict]:
        """Transcrever arquivo individual"""
        self._ensure_initialized()
        self._log_operation("transcri√ß√£o de arquivo", {"file_path": file_path})
        
        try:
            if not Path(file_path).exists():
                self.logger.error(f"Arquivo n√£o encontrado: {file_path}")
                return None
            
            # Carregar √°udio
            audio = whisper.load_audio(file_path)
            audio = whisper.pad_or_trim(audio)
            
            # Transcrever
            result = self.model.transcribe(
                audio,
                language=Config.WHISPER_LANGUAGE,
                word_timestamps=True,
                fp16=torch.cuda.is_available()
            )
            
            transcription = {
                'text': result['text'].strip(),
                'segments': result.get('segments', []),
                'language': result.get('language', Config.WHISPER_LANGUAGE),
                'file_path': file_path,
                'duration': len(audio) / 16000,
                'confidence': self._calculate_confidence(result.get('segments', [])),
                'transcribed_at': datetime.now().isoformat()
            }
            
            self._log_success("transcri√ß√£o de arquivo", {
                "text_length": len(transcription['text']),
                "confidence": transcription['confidence']
            })
            
            return transcription
            
        except Exception as e:
            self._log_error("transcri√ß√£o de arquivo", e)
            return None
    
    def transcribe_batch(self, audio_files: List[str]) -> List[Optional[Dict]]:
        """Transcrever m√∫ltiplos arquivos em batch"""
        self._log_operation("transcri√ß√£o em batch", {"file_count": len(audio_files)})
        
        results = []
        batch_size = Config.GPU_BATCH_SIZE
        
        for i in range(0, len(audio_files), batch_size):
            batch = audio_files[i:i + batch_size]
            batch_results = self._transcribe_batch_gpu(batch)
            results.extend(batch_results)
        
        successful = len([r for r in results if r is not None])
        self._log_success("transcri√ß√£o em batch", {
            "total": len(audio_files),
            "successful": successful,
            "failed": len(audio_files) - successful
        })
        
        return results
    
    def _transcribe_batch_gpu(self, audio_files: List[str]) -> List[Optional[Dict]]:
        """Transcrever batch na GPU"""
        results = []
        
        for file_path in audio_files:
            try:
                result = self.transcribe_file(file_path)
                results.append(result)
            except Exception as e:
                self.logger.error(f"Erro na transcri√ß√£o de {file_path}: {e}")
                results.append(None)
        
        return results
    
    def _calculate_confidence(self, segments: List[Dict]) -> float:
        """Calcular confian√ßa m√©dia da transcri√ß√£o"""
        if not segments:
            return 0.0
        
        confidences = []
        for segment in segments:
            if 'avg_logprob' in segment:
                # Converter log probability para confian√ßa (0-1)
                confidence = min(1.0, max(0.0, (segment['avg_logprob'] + 1.0)))
                confidences.append(confidence)
        
        return sum(confidences) / len(confidences) if confidences else 0.0
    
    def get_gpu_info(self) -> Dict[str, Any]:
        """Obter informa√ß√µes da GPU com fallback para CPU"""
        self._ensure_initialized()
        
        try:
            if torch.cuda.is_available():
                return {
                    'available': True,
                    'device_name': torch.cuda.get_device_name(),
                    'total_memory': torch.cuda.get_device_properties(0).total_memory,
                    'allocated_memory': torch.cuda.memory_allocated(),
                    'cached_memory': torch.cuda.memory_reserved(),
                    'memory_fraction': Config.GPU_MEMORY_FRACTION,
                    'device_type': 'GPU'
                }
            else:
                return {
                    'available': False,
                    'device_name': 'CPU',
                    'device_type': 'CPU',
                    'message': 'GPU n√£o dispon√≠vel - usando CPU (compat√≠vel com todos os sistemas)'
                }
        except Exception as e:
            return {
                'available': False,
                'device_name': 'CPU',
                'device_type': 'CPU',
                'error': str(e),
                'message': 'Erro ao detectar GPU - usando CPU'
            }
    
    def test_transcription(self, file_path: str) -> bool:
        """Testar transcri√ß√£o com arquivo"""
        self.logger.info(f"üß™ Testando transcri√ß√£o: {file_path}")
        
        try:
            result = self.transcribe_file(file_path)
            if result:
                self.logger.info(f"‚úÖ Teste OK - Texto: {result['text'][:100]}...")
                return True
            else:
                self.logger.error("‚ùå Teste falhou")
                return False
        except Exception as e:
            self.logger.error(f"‚ùå Erro no teste: {e}")
            return False

    def save_transcription_to_json(self, conversation_id: str, message_id: str, 
                                 transcription_data: Dict) -> Optional[str]:
        """Salvar transcri√ß√£o em arquivo JSON no mesmo padr√£o dos downloads"""
        self._ensure_initialized()
        
        try:
            # Criar diret√≥rio da conversa
            conv_dir = Config.TRANSCRIPTIONS_DIR / conversation_id
            conv_dir.mkdir(exist_ok=True)
            
            # Caminho do arquivo JSON
            json_path = conv_dir / f"{message_id}.json"
            
            # Se j√° existe, retornar
            if json_path.exists():
                self.logger.info(f"Transcri√ß√£o j√° existe: {json_path.name}")
                return str(json_path)
            
            # Preparar dados para salvar
            save_data = {
                "conversation_id": conversation_id,
                "message_id": message_id,
                "transcription": transcription_data,
                "created_at": datetime.now().isoformat(),
                "whisper_model": Config.WHISPER_MODEL,
                "device": self.device
            }
            
            # Salvar JSON
            with open(json_path, 'w', encoding='utf-8') as f:
                json.dump(save_data, f, ensure_ascii=False, indent=2)
            
            self.logger.info(f"‚úÖ Transcri√ß√£o salva: {json_path.name}")
            return str(json_path)
            
        except Exception as e:
            self.logger.error(f"‚ùå Erro ao salvar transcri√ß√£o: {e}")
            return None

    def load_transcription_from_json(self, conversation_id: str, message_id: str) -> Optional[Dict]:
        """Carregar transcri√ß√£o de arquivo JSON"""
        self._ensure_initialized()
        
        try:
            json_path = Config.TRANSCRIPTIONS_DIR / conversation_id / f"{message_id}.json"
            
            if not json_path.exists():
                return None
            
            with open(json_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            return data.get('transcription')
            
        except Exception as e:
            self.logger.error(f"‚ùå Erro ao carregar transcri√ß√£o: {e}")
            return None
